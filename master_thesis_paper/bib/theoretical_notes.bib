
@article{hangelbroek_nonlinear_2010,
	title = {Nonlinear approximation using {Gaussian} kernels},
	volume = {259},
	issn = {0022-1236},
	url = {http://www.sciencedirect.com/science/article/pii/S0022123610000467},
	doi = {10.1016/j.jfa.2010.02.001},
	abstract = {It is well known that nonlinear approximation has an advantage over linear schemes in the sense that it provides comparable approximation rates to those of the linear schemes, but to a larger class of approximands. This was established for spline approximations and for wavelet approximations, and more recently by DeVore and Ron (in press) [2] for homogeneous radial basis function (surface spline) approximations. However, no such results are known for the Gaussian function, the preferred kernel in machine learning and several engineering problems. We introduce and analyze in this paper a new algorithm for approximating functions using translates of Gaussian functions with varying tension parameters. At heart it employs the strategy for nonlinear approximation of DeVoreâ€“Ron, but it selects kernels by a method that is not straightforward. The crux of the difficulty lies in the necessity to vary the tension parameter in the Gaussian function spatially according to local information about the approximand: error analysis of Gaussian approximation schemes with varying tension are, by and large, an elusive target for approximators. We show that our algorithm is suitably optimal in the sense that it provides approximation rates similar to other established nonlinear methodologies like spline and wavelet approximations. As expected and desired, the approximation rates can be as high as needed and are essentially saturated only by the smoothness of the approximand.},
	language = {en},
	number = {1},
	urldate = {2020-02-28},
	journal = {Journal of Functional Analysis},
	author = {Hangelbroek, Thomas and Ron, Amos},
	month = jul,
	year = {2010},
	keywords = {Theory},
	pages = {203--219},
	file = {Hangelbroek und Ron - 2010 - Nonlinear approximation using Gaussian kernels.pdf:C\:\\Users\\Admin\\OneDrive\\FH Mechatronik\\Literaturquellen\\storage\\UQJXB5NZ\\Hangelbroek und Ron - 2010 - Nonlinear approximation using Gaussian kernels.pdf:application/pdf}
}

@article{park_universal_1991,
	title = {Universal {Approximation} {Using} {Radial}-{Basis}-{Function} {Networks}},
	volume = {3},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1991.3.2.246},
	doi = {10.1162/neco.1991.3.2.246},
	abstract = {There have been several recent studies concerning feedforward networks and the problem of approximating arbitrary functionals of a finite number of real variables. Some of these studies deal with cases in which the hidden-layer nonlinearity is not a sigmoid. This was motivated by successful applications of feedforward networks with nonsigmoidal hidden-layer units. This paper reports on a related study of radial-basis-function (RBF) networks, and it is proved that RBF networks having one hidden layer are capable of universal approximation. Here the emphasis is on the case of typical RBF networks, and the results show that a certain class of RBF networks with the same smoothing factor in each kernel node is broad enough for universal approximation.},
	number = {2},
	urldate = {2020-08-11},
	journal = {Neural Computation},
	author = {Park, J. and Sandberg, I. W.},
	month = jun,
	year = {1991},
	note = {Publisher: MIT Press},
	pages = {246--257},
	file = {Park und Sandberg - 1991 - Universal Approximation Using Radial-Basis-Functio.pdf:C\:\\Users\\Admin\\OneDrive\\FH Mechatronik\\Literaturquellen\\storage\\KWRZ849T\\Park und Sandberg - 1991 - Universal Approximation Using Radial-Basis-Functio.pdf:application/pdf}
}
