\documentclass[./\jobname.tex]{subfiles}
\begin{document}

\section{State of the Art}
\label{chap:state_of_the_art}

\subsection{Finite Element Method}
Currently, the Finite Element Method is the go-to approach to solve partial differential equations. The domain $\Omega$ on which the \gls{pde} is posed, is discretised into multiple smaller elements - as the name suggests. Thus, \gls{fem} counts to the category of meshed methods. The underlying solution function $u(\mathbf{x})$ to the PDE is then approximated by so called ``basis-functions'' $\Phi(\mathbf{x})$ limited to these finite elements. This thesis uses the open-source Netgen/NGSolve \gls{fem} package \cite{schoberl_ngsolvengsolve_2020}.\\
The general steps taken to solve a PDE with an FEM solver are: 
\begin{enumerate}
	\item \underline{Step: Strong Form} \\
		  This is the standard formulation of the linear \gls{pde}. $\mathbf{L}$ and $\mathbf{B}$ are linear differential operators that include the derivatives. \\
		  \begin{equation}
		  \label{eq: strong form}
			  \begin{split}
			  	\mathbf{x} \in \mathbb{R}^2 \\
			  	u(\mathbf{x}), f(\mathbf{x}), g(\mathbf{x}): \Omega \rightarrow \mathbb{R} \\
				\mathbf{L} u(\mathbf{x}) = f(\mathbf{x}) \text{ on $\Omega$} \\
				\mathbf{B} u(\mathbf{x}) = g(\mathbf{x}) \text{ on $\partial \Omega$}
			  \end{split}
		  \end{equation}
		  Further, only Dirichlet boundary conditions are considered, thus the boundary operator is always the identity matrix $\mathbf{B} = \mathbb{I}$. Therefore, the linear operator on the boundary $\mathbf{B}$ can be disregarded, resulting in  \\
		  \begin{equation}
		  	u(\mathbf{x})|_{\partial \Omega} = g(\mathbf{x}) .
		  \end{equation}
		  
	\item \underline{Step: Weak Form} \\
		  The next step is to reformulate the strong form into a usable weak form. This is equivalent to the strong form but written in an integral notation. In this equation, the $A$, $b$ and $c$ correspond to the constant factors of the derivatives in strong form. For the sake of completeness, this is kept abstract. In the \gls{pde}s considered in this work, $A = \mathbb{I}$ and $b=\mathbf{0}$, $c = 0$. Currently, the so-called test-function $v(\mathbf{x})$ is an arbitrary function, but it has to be 0 on the boundary $v(\mathbf{x})|_{\Omega} = 0$. The choice of the test-function correspond to different \gls{fem} types (\cite[p. 6f]{shen_spectral_2011}).\\
		  \begin{equation}
		  \label{eq: weak form}
		  \begin{split}
			  a(u,v) &
			  \begin{cases}
				  & ~ \int_{\Omega} - (\nabla^T A \nabla) u(\mathbf{x}) v(\mathbf{x}) dV \\ 
				  & - \int_{\Omega} b^T \nabla u(\mathbf{x}) v(\mathbf{x}) dV \\
				  & + \int_{\Omega} c u(\mathbf{x}) v(\mathbf{x}) dV
			  \end{cases} \\
			  F(v) &
			  \begin{cases} 
			  	& = \int_{\Omega} f(\mathbf{x}) v(\mathbf{x}) dV
			  \end{cases}
		  \end{split}
		  \end{equation} 
	\item \underline{Step: Discretisation of $\Omega$} \\
		  Create a mesh of finite elements that span the whole domain. Usually these are triangles. Thus, this step is sometimes called ``triangulation''.
	\item \underline{Step: Basis functions} \\
		  Choose a basis function $\Phi(\mathbf{x})$ that can be used to approximate the solution $u(\mathbf{x}) \approx u_{h}(\mathbf{x}) = \sum_{i = 1}^{N} u_i \Phi_i(\mathbf{x})$. A common choice are Lagrange or Chebyshev polynomials. In the Galerkin type \gls{fem}, the test-function $v(\mathbf{x})$ is the same as the trail-function, thus $v(\mathbf{x}) = \sum_{j = 1}^{N} v_j \Phi_j(\mathbf{x})$. The choice of the basis function $\Phi(\mathbf{x})$ largely influences the computational effort.  $\Phi(\mathbf{x})$ should have a small support, to produce a thinly populated matrix $A$ in the linear system of equations \eqref{eq:linear_system_of_equations} below.
	\item \underline{Step: Solution} \\
		  In the weak form, as seen in equation \eqref{eq: weak form}, $a(u,v)$ is a continuous bilinear form and $F(v)$ is a continuous linear functional. Substituting $u$ and $v$ with their corresponding approximation from \mbox{step 4} results in 
		  \begin{equation}
		  \sum_{j=1}^{N} v_j \sum_{i=1}^{N} u_i a(\Phi_i, \Phi_j) = \sum_{j=1}^{N} v_j F(\Phi_j).
		  \end{equation} 
		  Dividing by the $v_j$ values on both sides results in a linear system of equations, where the constant factors $u_i$ need to be determined.  
		  \begin{equation}
		  \label{eq:linear_system_of_equations}
		  \underbrace{\sum_{i=1}^{N} u_i a(\Phi_i, \Phi_j)}_{\mathbf{A u}} = \underbrace{F(\Phi_j)}_{\mathbf{b}} \text{ for $j=1,...N$}
		  \end{equation}
\end{enumerate}

Modern solvers include more complex and advanced techniques to further improve the solution error and the computation time. Some of the most important concepts that are also available in NGSolve are listed here. 

\begin{itemize}
	\item \underline{Static Condensation}: \\
		  Depending on the number of discrete elements, the $\mathbf{A}$ matrix can be very large. Inverting large matrices is very time consuming. Static condensation, also called Guyan reduction (\cite{guyan_reduction_1965}), reduces this dimensionality by exploiting the structure of $\mathbf{A}$. 
	\item \underline{Preconditioner}: \\
		  Instead of solving the $\mathbf{A}^{-1}$ exactly, this can also be approximated by a matrix that is similar to $\mathbf{A}^{-1}$. The actual inverse can be iteratively approximated. NGSolve implements multiple different preconditioners and it even allows to create your own method. 
	\item \underline{Adaptive Mesh Refinement}: \\
		The accuracy of a FEM-approximated solution mainly depends on the density of the mesh. Typically, finer meshes tend to produce more accurate solutions, but the computation time is longer. This trade-off can be overcome by a self-adaptive mesh. NGSolve implements that in an adaptive loop that executes: 
		\begin{itemize}
			\item Solve PDE (with coarse mesh)
			\item Estimate Error (for every element)
			\item Mark Elements (that have the greatest error)
			\item Refine Elements (that were previously marked)
			\item Repeat until \gls{dof} exceed a specified $N$
		\end{itemize}
\end{itemize}


\subsection{Computational Intelligence Methods} 
\label{chap:literature_overview}
The following table \ref{tab:literature_research} gives a brief overview of these papers and sorts them historically. In general, all of these papers from the table use the \gls{wrm}, or some variant of that concept, to transform their differential equation into an optimisation problem. This serves as the fitness function and is necessary to evaluate a possible candidate solution and perform the evolutionary selection. The fitness function is also called objective function and these terms are used interchangeably in this paper. In short, the residual $R$ is defined through the differential equation itself and can be calculated by $R(u(\mathbf{x})) = \mathbf{L}u(\mathbf{x}) - f(\mathbf{x})$. The residual can be thought of as a functional that substitutes $u(\mathbf{x})$ with an approximate solution $u_{apx}(\mathbf{x})$ and returns a numerical score. \\
This paper mainly builds on the work presented in \cite{chaquet_using_2019}. In their paper they describe an algorithm that approximates a solution with a linear combination of Gaussian \gls{rbf} as kernels:
\begin{equation}
u(\mathbf{x})_{apx} = \sum_{i=1}^{N} \omega_i e^{\gamma_i (\left||\mathbf{x} - \mathbf{c}_i\right||^2)}
\end{equation}
The approximated function $u(\mathbf{x})_{apx}$ can be fully determined by a finite number of parameters: $\omega_i, \gamma_i, \mathbf{c}_i$. These are stacked together into a vector $\mathbf{p_{apx}}$ and called the decision variables which are optimised by the algorithm. The objective function can be seen in equation \eqref{eq:fit_func_chaquet}. 
\begin{equation}
\label{eq:fit_func_chaquet}
\begin{split}
F(u_{apx}(\mathbf{x})) = \frac{1}{m (n_C + n_B)} \\ \left[ \sum_{i=1}^{n_C} \right. \xi (\mathbf{x}_i) || \mathbf{L}u_{apx}(\mathbf{x}_i) - f(\mathbf{x}_i)||^2 \\ + \left. \phi \sum_{j=1}^{n_B} || \mathbf{B}u_{apx}(\mathbf{x}_j) - g(\mathbf{x}_j)||^2 \right] 
\end{split}
\end{equation}
The multipliers $\xi(\mathbf{x}_i)$ and $\phi$ are weighting factors for either the inner or the boundary term. The whole term is normalised with the number of collocation points. The parameters of the kernels are determined via a \gls{cma_es} \cite{hansen_reducing_2003}. To further improve the solution, the evolutionary algorithm is coupled with a \gls{ds} method to carry out the local search. The authors show empirically that the local search significantly improves the performance by testing the algorithm on a set of 32 differential equations. 

\begin{table}[h]
	\centering
	\noindent\adjustbox{max width=\linewidth}{
		\begin{tabular}{|c|c|c|c|}
			
			\hline
			\rowcolor[HTML]{\farbeTabA}
			
			Paper & Algorithm & Representation & Problems \\ \hline
			
			\multilinecell{\cite{howard_genetic_2001}} & \multilinecell{GP} & \multilinecell{polynomial of \\ arbitrary length} & \multilinecell{one-dimensional \\ steady-state \\ model of \\ convection-diffusion \\ equation} \\ \hline
			
			\multilinecell{\cite{kirstukas_hybrid_2005}} & \multilinecell{GP} & \multilinecell{algebraic \\ expression} & \multilinecell{heating of thin rod \\ heating by current} \\ \hline
			
			\multilinecell{\cite{tsoulos_solving_2006}} & \multilinecell{GE} & \multilinecell{algebraic term} & \multilinecell{set of ODEs \\ system of ODEs \\ and PDEs} \\ \hline
			
			\multilinecell{\cite{mastorakis_unstable_2006}} & \multilinecell{GA\\(global); \\ DS\\(local)} & \multilinecell{5th order \\ polynomial}& \multilinecell{unstable \\ ODEs} \\ \hline
			
			\multilinecell{\cite{sobester_genetic_2008}} & \multilinecell{GP \\ and \\ RBF-NN} & \multilinecell{algebraic term \\ for inner; \\ RBF for boundary} & elliptic PDEs \\ \hline
			
			\multilinecell{\cite{howard_genetic_2011}} & \multilinecell{GP} & function value grid & \multilinecell{convectionâ€“diffusion \\ equation \\ at different \\ Peclet numbers } \\ \hline
			
			\multilinecell{\cite{chaquet_solving_2012}} & \multilinecell{ES} & \multilinecell{partial sum \\ of Fourier series} & \multilinecell{testbench of \\ ODEs \\ system of ODEs \\ and PDEs} \\ \hline
			
			\multilinecell{\cite{babaei_general_2013}} & \multilinecell{PSO} & \multilinecell{partial sum\\of Fourier series} & \multilinecell{integro-differential equation\\system of linear ODEs \\ Brachistochrone \\ nonlinear Bernoulli} \\ \hline
			
			\multilinecell{\cite{panagant_solving_2014}} & \multilinecell{DE} & \multilinecell{polynomial of \\ unspecified order} & \multilinecell{set of 6 \\ different PDEs}  \\ \hline
			
			\multilinecell{\cite{sadollah_metaheuristic_2017}} & \multilinecell{PSO\\HS\\WCA} & \multilinecell{partial sum\\of Fourier series} & \multilinecell{singular BVP} \\ \hline
			
			\multilinecell{\cite{chaquet_using_2019}} & \multilinecell{CMA-ES\\(global); \\ DS\\(local)} & \multilinecell{linear combination \\ of Gaussian kernels} & \multilinecell{testbench of \\ ODEs \\ system of ODEs \\ and PDEs}\\ \hline
			
			\multilinecell{\cite{fateh_differential_2019}} & \multilinecell{DE} & \multilinecell{function value\\grid} & elliptic PDEs \\ \hline
			
		\end{tabular}
	}
	\unterschrift{Literature research on the general topic of stochastic solver and their application. The papers are sorted by date of release. }{}{}
	\label{tab:literature_research}
\end{table}


\subsection{Differential Evolution}

The differential evolution framework was first introduced in \cite{storn_differential_1997}. Due to its simple and flexible structure, it quickly became one of the most successful evolutionary algorithm. Over the years, several adaptations to the original framework have been proposed. Some of them currently count to the best performing algorithms, as the 100-Digit Challenge at GECCO 2019 \cite{suganthan_suganthancec2019_2020} shows. The main \gls{de} framework consists of three necessary steps that continuously update a population of possible solutions. The population can be interpreted as a matrix, where each row-vector $\mathbf{x}_i$, also called individual, represents a point within the search domain and has a fitness according to the fitness function $f(\mathbf{x}_i): \mathbb{R}^n \rightarrow \mathbb{R}$. The goal is to minimise the fitness function. These steps are performed in a loop until a predefined termination condition is reached. Each step is controlled by a user-defined parameter: 
\begin{itemize}
	\item \underline{Mutation}: \\
		  Mutation strength parameter F;\\
		  The mutation uses the information from within the population to create a trial vector $v_i$. This is done by scaling the difference between some vectors in the population. The \textit{/current-to-pbest/1} mutation operator can be seen in equation \eqref{eq:mut_rand_1} where $x_i$ is the current individual, $x_{best}^p$ is one random vector of the p\% top vectors, $x_{r1}$ is a random vector from the population while $\tilde{x}_{r2}$ is randomly chosen from the population and the archive. $x_{r1}$ and $\tilde{x}_{r2}$ must not describe the same individual.
		  \begin{equation}
		  \label{eq:mut_rand_1}
		  v_i = x_{i} + F_i(x_{best}^p - x_{i}) + F_i(x_{r1} - \tilde{x}_{r2})
		  \end{equation}
	\item \underline{Crossover}: \\
		  Crossover probability parameter CR;\\
		  The crossover procedure randomly mixes the information between the trial vector $v_i$ and a random candidate from the population $x_{i}$ to create a new trial vector $u_i$. The binomial crossover from equation \eqref{eq:crs_bin} randomly takes elements from both vectors, where $K$ is a random index to ensure that at least one element from the trial vector $v_i$ is taken.
		  \begin{equation}
		  \label{eq:crs_bin}
		  u_{ij}=\begin{cases}
		  v_{ij}, &\text{if $j = K \lor rand[0,1] \leq CR$}\\
		  x_{ij}, &\text{otherwise}
		  \end{cases}
		  \end{equation}
	\item \underline{Selection}: \\
		  Population size N;\\
		  The selection replaces the old candidate $x_i$ if the trial candidate $u_i$ is better as measured by the fitness function. This is performed for every individual in the population, then the next generation is started.
\end{itemize}  ~\\
In modern \gls{de} variants, these parameters are self-adapted during the evolutionary process. This means that the algorithms can balance out between exploration of the search-space and exploitation of promising locations. A prominent example of a modern \gls{de} with self-adaption is JADE, which is described in \cite{zhang_jade_2009}. The adaption is performed by taking successful F and CR values of the last generation into account. If a certain setting is successful in generating better candidates, newly selected F and CR gravitate towards that setting. 


\end{document}
